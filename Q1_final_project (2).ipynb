{
 "nbformat": 4,
 "nbformat_minor": 0,
 "metadata": {
  "colab": {
   "name": "Q1_final_project.ipynb",
   "provenance": [],
   "collapsed_sections": [],
   "authorship_tag": "ABX9TyMs7fcH1fq6qHTMP4jyEWTU"
  },
  "kernelspec": {
   "name": "pycharm-a1321954",
   "language": "python",
   "display_name": "PyCharm (subspace-clustering-code)"
  }
 },
 "cells": [
  {
   "cell_type": "code",
   "metadata": {
    "id": "7zguO8AJMjcD",
    "colab_type": "code",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 322
    },
    "executionInfo": {
     "status": "ok",
     "timestamp": 1600241304332,
     "user_tz": -180,
     "elapsed": 64892,
     "user": {
      "displayName": "nyueygho4 מהמיקמק",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Ggqc1SFhXUXd-_5m0Cmd6C6GdJ3rMiD1y_E2jB6gw=s64",
      "userId": "01963776509194069257"
     }
    },
    "outputId": "b04f1a59-0baa-46fd-9165-c57390af1ce3"
   },
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import scipy as sc\n",
    "from scipy.stats import randint,norm,multivariate_normal, ortho_group\n",
    "from scipy import linalg\n",
    "from scipy.linalg import subspace_angles,orth\n",
    "from scipy.optimize import fmin,linear_sum_assignment\n",
    "from scipy import optimize\n",
    "from scipy.optimize import minimize\n",
    "import math\n",
    "from statistics import mean, stdev ,variance\n",
    "import seaborn as sns\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.decomposition import PCA\n",
    "import itertools as it\n",
    "import matplotlib.pyplot as plt\n",
    "!pip install spams\n",
    "from google.colab import files\n",
    "import sys\n",
    "\n"
   ],
   "execution_count": 3,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting spams\r\n",
      "  Using cached spams-2.6.2.5.tar.gz (1.6 MB)\r\n",
      "Requirement already satisfied: Cython>=0.29 in /Users/deborahwolhandler/.virtualenvs/stat/lib/python3.7/site-packages (from spams) (0.29.21)\r\n",
      "Requirement already satisfied: numpy>=1.12 in /Users/deborahwolhandler/.virtualenvs/stat/lib/python3.7/site-packages (from spams) (1.19.1)\r\n",
      "Requirement already satisfied: Pillow>=6.0 in /Users/deborahwolhandler/.virtualenvs/stat/lib/python3.7/site-packages (from spams) (7.2.0)\r\n",
      "Requirement already satisfied: scipy>=1.0 in /Users/deborahwolhandler/.virtualenvs/stat/lib/python3.7/site-packages (from spams) (1.5.2)\r\n",
      "Requirement already satisfied: six>=1.12 in /Library/Developer/CommandLineTools/Library/Frameworks/Python3.framework/Versions/3.7/lib/python3.7/site-packages (from spams) (1.12.0)\r\n",
      "Building wheels for collected packages: spams\r\n",
      "  Building wheel for spams (setup.py) ... \u001B[?25l-\b \b\\"
     ]
    },
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'google'",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mModuleNotFoundError\u001B[0m                       Traceback (most recent call last)",
      "\u001B[0;32m<ipython-input-3-3c43c398e9ce>\u001B[0m in \u001B[0;36m<module>\u001B[0;34m\u001B[0m\n\u001B[1;32m     16\u001B[0m \u001B[0;32mimport\u001B[0m \u001B[0mmatplotlib\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mpyplot\u001B[0m \u001B[0;32mas\u001B[0m \u001B[0mplt\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     17\u001B[0m \u001B[0mget_ipython\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0msystem\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m'pip install spams'\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m---> 18\u001B[0;31m \u001B[0;32mfrom\u001B[0m \u001B[0mgoogle\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mcolab\u001B[0m \u001B[0;32mimport\u001B[0m \u001B[0mfiles\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m     19\u001B[0m \u001B[0;32mimport\u001B[0m \u001B[0msys\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     20\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;31mModuleNotFoundError\u001B[0m: No module named 'google'"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "#@title ElasticNetSubspaceClustering Code\n",
    "import warnings\n",
    "import progressbar\n",
    "import spams\n",
    "import time\n",
    "\n",
    "from scipy import sparse\n",
    "from sklearn import cluster\n",
    "from sklearn.base import BaseEstimator, ClusterMixin\n",
    "from sklearn.decomposition import sparse_encode\n",
    "from sklearn.linear_model import orthogonal_mp\n",
    "from sklearn.neighbors import kneighbors_graph\n",
    "from sklearn.preprocessing import normalize\n",
    "from sklearn.utils import check_random_state, check_array, check_symmetric\n",
    "\n",
    "\n",
    "class SelfRepresentation(BaseEstimator, ClusterMixin):\n",
    "\n",
    "    def __init__(self, n_clusters=8, affinity='symmetrize', random_state=None, n_init=20, n_jobs=1):\n",
    "        self.n_clusters = n_clusters\n",
    "        self.affinity = affinity\n",
    "        self.random_state = random_state\n",
    "        self.n_init = n_init\n",
    "        self.n_jobs = n_jobs\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        X = check_array(X, accept_sparse=['csr', 'csc', 'coo'], dtype=np.float64)\n",
    "        time_base = time.time()\n",
    "\n",
    "        self._self_representation(X)\n",
    "        self.timer_self_representation_ = time.time() - time_base\n",
    "\n",
    "        self._representation_to_affinity()\n",
    "        self._spectral_clustering()\n",
    "        self.timer_time_ = time.time() - time_base\n",
    "\n",
    "        return self\n",
    "\n",
    "    def fit_self_representation(self, X, y=None):\n",
    "        X = check_array(X, accept_sparse=['csr', 'csc', 'coo'], dtype=np.float64)\n",
    "        time_base = time.time()\n",
    "\n",
    "        self._self_representation(X)\n",
    "        self.timer_self_representation_ = time.time() - time_base\n",
    "\n",
    "        return self\n",
    "\n",
    "    def _representation_to_affinity(self):\n",
    "        normalized_representation_matrix_ = normalize(self.representation_matrix_, 'l2')\n",
    "        if self.affinity == 'symmetrize':\n",
    "            self.affinity_matrix_ = 0.5 * (np.absolute(normalized_representation_matrix_) + np.absolute(normalized_representation_matrix_.T))\n",
    "        elif self.affinity == 'nearest_neighbors':\n",
    "            neighbors_graph = kneighbors_graph(normalized_representation_matrix_, 3,\n",
    "\t\t                                       mode='connectivity', include_self=False)\n",
    "            self.affinity_matrix_ = 0.5 * (neighbors_graph + neighbors_graph.T)\n",
    "\n",
    "    def _spectral_clustering(self):\n",
    "        affinity_matrix_ = check_symmetric(self.affinity_matrix_)\n",
    "        random_state = check_random_state(self.random_state)\n",
    "\n",
    "        laplacian = sparse.csgraph.laplacian(affinity_matrix_, normed=True)\n",
    "        _, vec = sparse.linalg.eigsh(sparse.identity(laplacian.shape[0]) - laplacian,\n",
    "                                     k=self.n_clusters, sigma=None, which='LA')\n",
    "        embedding = normalize(vec)\n",
    "        _, self.labels_, _ = cluster.k_means(embedding, self.n_clusters,\n",
    "                                             random_state=random_state, n_init=self.n_init)\n",
    "\n",
    "\n",
    "def active_support_elastic_net(X, y, alpha, tau=1.0, algorithm='spams', support_init='knn',\n",
    "                               support_size=100, maxiter=40):\n",
    "    n_samples = X.shape[0]\n",
    "\n",
    "    if n_samples <= support_size:  # skip active support search for small scale data\n",
    "        supp = np.arange(n_samples, dtype=int)  # this results in the following iteration to converge in 1 iteration\n",
    "    else:\n",
    "        if support_init == 'L2':\n",
    "            L2sol = np.linalg.solve(np.identity(y.shape[1]) * alpha + np.dot(X.T, X), y.T)\n",
    "            c0 = np.dot(X, L2sol)[:, 0]\n",
    "            supp = np.argpartition(-np.abs(c0), support_size)[0:support_size]\n",
    "        elif support_init == 'knn':\n",
    "            supp = np.argpartition(-np.abs(np.dot(y, X.T)[0]), support_size)[0:support_size]\n",
    "\n",
    "    curr_obj = float(\"inf\")\n",
    "    for _ in range(maxiter):\n",
    "        Xs = X[supp, :]\n",
    "        if algorithm == 'spams':\n",
    "            cs = spams.lasso(np.asfortranarray(y.T), D=np.asfortranarray(Xs.T),\n",
    "                             lambda1=tau*alpha, lambda2=(1.0-tau)*alpha)\n",
    "            cs = np.asarray(cs.todense()).T\n",
    "        else:\n",
    "            cs = sparse_encode(y, Xs, algorithm=algorithm, alpha=alpha)\n",
    "\n",
    "        delta = (y - np.dot(cs, Xs)) / alpha\n",
    "\n",
    "        obj = tau * np.sum(np.abs(cs[0])) + (1.0 - tau)/2.0 * np.sum(np.power(cs[0], 2.0)) + alpha/2.0 * np.sum(np.power(delta, 2.0))\n",
    "        if curr_obj - obj < 1.0e-10 * curr_obj:\n",
    "            break\n",
    "        curr_obj = obj\n",
    "\n",
    "        coherence = np.abs(np.dot(delta, X.T))[0]\n",
    "        coherence[supp] = 0\n",
    "        addedsupp = np.nonzero(coherence > tau + 1.0e-10)[0]\n",
    "\n",
    "        if addedsupp.size == 0:  # converged\n",
    "            break\n",
    "\n",
    "        # Find the set of nonzero entries of cs.\n",
    "        activesupp = supp[np.abs(cs[0]) > 1.0e-10]\n",
    "\n",
    "        if activesupp.size > 0.8 * support_size:  # this suggests that support_size is too small and needs to be increased\n",
    "            support_size = min([round(max([activesupp.size, support_size]) * 1.1), n_samples])\n",
    "\n",
    "        if addedsupp.size + activesupp.size > support_size:\n",
    "            ord = np.argpartition(-coherence[addedsupp], support_size - activesupp.size)[0:support_size - activesupp.size]\n",
    "            addedsupp = addedsupp[ord]\n",
    "\n",
    "        supp = np.concatenate([activesupp, addedsupp])\n",
    "\n",
    "    c = np.zeros(n_samples)\n",
    "    c[supp] = cs\n",
    "    return c\n",
    "\n",
    "\n",
    "def elastic_net_subspace_clustering(X, gamma=50.0, gamma_nz=True, tau=1.0, algorithm='lasso_lars',\n",
    "                                    active_support=True, active_support_params=None, n_nonzero=50):\n",
    "    if algorithm in ('lasso_lars', 'lasso_cd') and tau < 1.0 - 1.0e-10:\n",
    "        warnings.warn('algorithm {} cannot handle tau smaller than 1. Using tau = 1'.format(algorithm))\n",
    "        tau = 1.0\n",
    "\n",
    "    if active_support == True and active_support_params == None:\n",
    "        active_support_params = {}\n",
    "\n",
    "    n_samples = X.shape[0]\n",
    "    rows = np.zeros(n_samples * n_nonzero)\n",
    "    cols = np.zeros(n_samples * n_nonzero)\n",
    "    vals = np.zeros(n_samples * n_nonzero)\n",
    "    curr_pos = 0\n",
    "\n",
    "    for i in progressbar.progressbar(range(n_samples)):\n",
    "        y = X[i, :].copy().reshape(1, -1)\n",
    "        X[i, :] = 0\n",
    "\n",
    "        if algorithm in ('lasso_lars', 'lasso_cd', 'spams'):\n",
    "            if gamma_nz == True:\n",
    "                coh = np.delete(np.absolute(np.dot(X, y.T)), i)\n",
    "                alpha0 = np.amax(coh) / tau  # value for which the solution is zero\n",
    "                alpha = alpha0 / gamma\n",
    "            else:\n",
    "                alpha = 1.0 / gamma\n",
    "\n",
    "            if active_support == True:\n",
    "                c = active_support_elastic_net(X, y, alpha, tau, algorithm, **active_support_params)\n",
    "            else:\n",
    "                if algorithm == 'spams':\n",
    "                    c = spams.lasso(np.asfortranarray(y.T), D=np.asfortranarray(X.T),\n",
    "                                    lambda1=tau * alpha, lambda2=(1.0-tau) * alpha)\n",
    "                    c = np.asarray(c.todense()).T[0]\n",
    "                else:\n",
    "                    c = sparse_encode(y, X, algorithm=algorithm, alpha=alpha)[0]\n",
    "        else:\n",
    "          warnings.warn(\"algorithm {} not found\".format(algorithm))\n",
    "\n",
    "        index = np.flatnonzero(c)\n",
    "        if index.size > n_nonzero:\n",
    "        #  warnings.warn(\"The number of nonzero entries in sparse subspace clustering exceeds n_nonzero\")\n",
    "          index = index[np.argsort(-np.absolute(c[index]))[0:n_nonzero]]\n",
    "        rows[curr_pos:curr_pos + len(index)] = i\n",
    "        cols[curr_pos:curr_pos + len(index)] = index\n",
    "        vals[curr_pos:curr_pos + len(index)] = c[index]\n",
    "        curr_pos += len(index)\n",
    "\n",
    "        X[i, :] = y\n",
    "\n",
    "#   affinity = sparse.csr_matrix((vals, (rows, cols)), shape=(n_samples, n_samples)) + sparse.csr_matrix((vals, (cols, rows)), shape=(n_samples, n_samples))\n",
    "    return sparse.csr_matrix((vals, (rows, cols)), shape=(n_samples, n_samples))\n",
    "class ElasticNetSubspaceClustering(SelfRepresentation):\n",
    "    def __init__(self, n_clusters=8, affinity='symmetrize', random_state=None, n_init=20, n_jobs=1, gamma=50.0, gamma_nz=True, tau=1.0,\n",
    "                 algorithm='lasso_lars', active_support=True, active_support_params=None, n_nonzero=50):\n",
    "        self.gamma = gamma\n",
    "        self.gamma_nz = gamma_nz\n",
    "        self.tau = tau\n",
    "        self.algorithm = algorithm\n",
    "        self.active_support = active_support\n",
    "        self.active_support_params = active_support_params\n",
    "        self.n_nonzero = n_nonzero\n",
    "\n",
    "        SelfRepresentation.__init__(self, n_clusters, affinity, random_state, n_init, n_jobs)\n",
    "\n",
    "    def _self_representation(self, X):\n",
    "        self.representation_matrix_ = elastic_net_subspace_clustering(X, self.gamma, self.gamma_nz,\n",
    "                                                                      self.tau, self.algorithm,\n",
    "\t\t                                                              self.active_support, self.active_support_params,\n",
    "\t\t                                                              self.n_nonzero)\n",
    "\n",
    "\n",
    "def sparse_subspace_clustering_orthogonal_matching_pursuit(X, n_nonzero=10, thr=1.0e-6):\n",
    "    n_samples = X.shape[0]\n",
    "    rows = np.zeros(n_samples * n_nonzero, dtype = int)\n",
    "    cols = np.zeros(n_samples * n_nonzero, dtype = int)\n",
    "    vals = np.zeros(n_samples * n_nonzero)\n",
    "    curr_pos = 0\n",
    "\n",
    "    for i in progressbar.progressbar(range(n_samples)):\n",
    "    # for i in range(n_samples):\n",
    "        residual = X[i, :].copy()  # initialize residual\n",
    "        supp = np.empty(shape=(0), dtype = int)  # initialize support\n",
    "        residual_norm_thr = np.linalg.norm(X[i, :]) * thr\n",
    "        for t in range(n_nonzero):  # for each iteration of OMP\n",
    "            # compute coherence between residuals and X\n",
    "            coherence = abs( np.matmul(residual, X.T) )\n",
    "            coherence[i] = 0.0\n",
    "            # update support\n",
    "            supp = np.append(supp, np.argmax(coherence))\n",
    "            # compute coefficients\n",
    "            c = np.linalg.lstsq( X[supp, :].T, X[i, :].T, rcond=None)[0]\n",
    "            # compute residual\n",
    "            residual = X[i, :] - np.matmul(c.T, X[supp, :])\n",
    "            # check termination\n",
    "            if np.sum(residual **2) < residual_norm_thr:\n",
    "                break\n",
    "\n",
    "        rows[curr_pos:curr_pos + len(supp)] = i\n",
    "        cols[curr_pos:curr_pos + len(supp)] = supp\n",
    "        vals[curr_pos:curr_pos + len(supp)] = c\n",
    "        curr_pos += len(supp)\n",
    "\n",
    "#   affinity = sparse.csr_matrix((vals, (rows, cols)), shape=(n_samples, n_samples)) + sparse.csr_matrix((vals, (cols, rows)), shape=(n_samples, n_samples))\n",
    "    return sparse.csr_matrix((vals, (rows, cols)), shape=(n_samples, n_samples))\n",
    "\n",
    "\n",
    "class SparseSubspaceClusteringOMP(SelfRepresentation):\n",
    "    def __init__(self, n_clusters=8, affinity='symmetrize', random_state=None, n_init=10, n_jobs=1, n_nonzero=10, thr=1.0e-6):\n",
    "        self.n_nonzero = n_nonzero\n",
    "        self.thr = thr\n",
    "        SelfRepresentation.__init__(self, n_clusters, affinity, random_state, n_init, n_jobs)\n",
    "\n",
    "    def _self_representation(self, X):\n",
    "        self.representation_matrix_ = sparse_subspace_clustering_orthogonal_matching_pursuit(X, self.n_nonzero, self.thr)\n",
    "\n",
    "\n",
    "def least_squares_subspace_clustering(X, gamma=10.0, exclude_self=False):\n",
    "    n_samples, n_features = X.shape\n",
    "\n",
    "    if exclude_self == False:\n",
    "        if n_samples < n_features:\n",
    "            gram = np.matmul(X, X.T)\n",
    "            return np.linalg.solve(gram + np.eye(n_sample) / gamma, gram).T\n",
    "        else:\n",
    "            tmp = np.linalg.solve(np.matmul(X.T, X) + np.eye(n_features) / gamma, X.T)\n",
    "            return np.matmul(X, tmp).T\n",
    "    else:\n",
    "        if n_samples < n_features:\n",
    "            D = np.linalg.solve(np.matmul(X, X.T) + np.eye(n_sample) / gamma, np.eye(n_sample))\n",
    "            # see Theorem 6 in https://arxiv.org/pdf/1404.6736.pdf\n",
    "        else:\n",
    "            tmp = np.linalg.solve(np.matmul(X.T, X) + np.eye(n_features) / gamma, X.T)\n",
    "            D = eye(n_samples) - np.matmul(X, tmp)\n",
    "        D = D / D.diagonal()[None,:]\n",
    "        np.fill_diagonal(D, 0.0)\n",
    "        return -1.0 * D.T\n",
    "class LeastSquaresSubspaceClustering(SelfRepresentation):\n",
    "    def __init__(self, n_clusters=8, affinity='symmetrize', random_state=None, n_init=None, n_jobs=1, gamma=10.0, exclude_self=False):\n",
    "        self.gamma = gamma\n",
    "        self.exclude_self = exclude_self\n",
    "        SelfRepresentation.__init__(self, n_clusters, affinity, random_state, n_init, n_jobs)\n",
    "\n",
    "    def _self_representation(self, X):\n",
    "        self.representation_matrix_ = least_squares_subspace_clustering(X, self.gamma, self.exclude_self)\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gXfjqThkMmVj",
    "colab_type": "text"
   },
   "source": [
    "# 1. Simulation Study, Noiseless case. Simulate data according to the model above with the following parameters:\n",
    "- n = 2^3,2^4,...,2^10.\n",
    "- p = 2^4,2^5,2^6,2^ d 7.\n",
    "- d = 2^(−1)p, 2^(−2)p, 2^(−3)p,2^(−4)p, for each of the values of p.\n",
    "- K = 4 clusters.\n",
    "- θ = 10^(−2)θmax, 10^(−1)θmax, θmax, where θmax is the value obtained on average by taking the different subspaces Bi to have uniformly random orientations"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "i8XUg649_ZyH",
    "colab_type": "code",
    "colab": {},
    "executionInfo": {
     "status": "ok",
     "timestamp": 1600241378647,
     "user_tz": -180,
     "elapsed": 1326,
     "user": {
      "displayName": "nyueygho4 מהמיקמק",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Ggqc1SFhXUXd-_5m0Cmd6C6GdJ3rMiD1y_E2jB6gw=s64",
      "userId": "01963776509194069257"
     }
    }
   },
   "source": [
    "#for each subspace K unit vectors in random directions forming a basis\n",
    "def first_simulatin(p,dim,k):\n",
    "  b = [orth(np.random.randn(p, dim)) for k in range(k+1)]\n",
    "  return (b)\n",
    "\n",
    "\n",
    "#This yields an average pairwise angle denote θmax which depends on the dimensions p and d\n",
    "def find_theta_max(p,dim):\n",
    "  theta_max = []\n",
    "  for i in range(100):\n",
    "    rand_subspac1 = orth(np.random.randn(p, dim))\n",
    "    rand_subspac2 = orth(np.random.randn(p, dim))\n",
    "    theta_max.append(subspace_angles(rand_subspac1,rand_subspac2).max())\n",
    "  max_avg_theta = np.average(theta_max)\n",
    "  return(max_avg_theta)\n",
    "\n",
    "#Then, replace each Bi by a linear combination Bi ← αBi + (1 − α)B0 where α ∈ (0, 1) is calibrated to yield average pairwise angle θ\n",
    "def second_simulation(p,k,dim,theta,b) :\n",
    "  def find_a_for_theta(a,p=p, dim=dim,theta = theta) :\n",
    "    temp_theta = []\n",
    "    for i in range(100):\n",
    "      rand_subspac0 = orth(np.random.randn(p, dim))\n",
    "      rand_subspac1 = orth(np.random.randn(p, dim))\n",
    "      rand_subspac2 = orth(np.random.randn(p, dim))\n",
    "      temp_theta.append(subspace_angles(rand_subspac0*(1-a)+rand_subspac1*a,rand_subspac0*(1-a)+rand_subspac2*a).max())\n",
    "    return (np.average(temp_theta)-theta)\n",
    "  a= sc.optimize.bisect(find_a_for_theta,0,1)\n",
    "  B = [b[0]*(1-a)+b[i]*a for i in range(1,k+1)]\n",
    "  return (B)\n",
    "\n",
    "# consider the following generative model for the data: zi ∼ U({1, .., K}), wi ∼ N(0, Id), xi|zi, wi ∼ N(Bziwi, σ2Ip)\n",
    "def third_simulation(n,p,dim,B,k,theta) :\n",
    "  z = np.random.randint(0,k,n)\n",
    "  w = np.random.multivariate_normal(mean = np.zeros(dim),cov =np.diag(np.ones(dim)),size=n)\n",
    "  X= np.zeros((n,p))\n",
    "  for i in range(n):\n",
    "    X[i,] = np.random.multivariate_normal(mean = np.array(np.dot(np.matrix(w[i,:]),B[z[i]].T)).flatten(),cov = np.diag(0*np.ones(p)))\n",
    "  return (n,p,dim,theta,X,z,B)\n"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "Vm4pDOZjOu6T",
    "colab_type": "code",
    "colab": {},
    "executionInfo": {
     "status": "ok",
     "timestamp": 1600241382922,
     "user_tz": -180,
     "elapsed": 991,
     "user": {
      "displayName": "nyueygho4 מהמיקמק",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Ggqc1SFhXUXd-_5m0Cmd6C6GdJ3rMiD1y_E2jB6gw=s64",
      "userId": "01963776509194069257"
     }
    }
   },
   "source": [
    "#data simulation\n",
    "def final_data_simulation(k) :\n",
    "  nn = [2 ** j for j in range(3,11)]\n",
    "  pp = [2 ** j for j in range(4,8)]\n",
    "  dd = [2 ** -j for j in range(1,5)]\n",
    "  tt = [10 ** -j for j in range(0,3)]\n",
    "  df = pd.DataFrame(columns=['n', 'p', 'dim','theta','X','z','B'])\n",
    "  for p in pp:\n",
    "    for d in dd:\n",
    "      dim = int(d*p)\n",
    "      b= first_simulatin(p=p,dim=dim,k=k)\n",
    "      for t in tt :\n",
    "        theta= find_theta_max(p=p,dim=dim)*t\n",
    "        if (t==1) :\n",
    "          a=1\n",
    "          B = [b[0]*(1-a)+b[i]*a for i in range(1,k+1)]\n",
    "        else :\n",
    "          B= second_simulation(p,k,dim,theta,b)\n",
    "        for n in nn:\n",
    "          row=pd.Series(list(third_simulation(n=n,p=p,dim=dim,B=B,k=k,theta=theta)[0:7]),[\"n\",\"p\",\"dim\",\"theta\",\"X\",\"z\",\"B\"])\n",
    "          df= df.append([row],ignore_index=True)\n",
    "  return (df)\n",
    "\n"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "f9nGeOkiF6X-",
    "colab_type": "text"
   },
   "source": [
    "## **Clustering and subspacing: **\n",
    "You should pick one of the algorithms above (ENsc) in additon to the naive K-mean"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "vJWoZY74N7py",
    "colab_type": "code",
    "colab": {},
    "executionInfo": {
     "status": "ok",
     "timestamp": 1600241395106,
     "user_tz": -180,
     "elapsed": 1311,
     "user": {
      "displayName": "nyueygho4 מהמיקמק",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Ggqc1SFhXUXd-_5m0Cmd6C6GdJ3rMiD1y_E2jB6gw=s64",
      "userId": "01963776509194069257"
     }
    }
   },
   "source": [
    "#. After Algorithm (kmean and additional) clustering, which yields cluster identities ˆz1, .., zˆn,we estimate the sub-space of each cluster k by performing PCA on\n",
    "#the points in this clusterand keeping the top d components as a basis for Bˆk for k = 1, ..,\n",
    "def pca_subspace(df,i,dim):\n",
    "  df_new= df[df['cluster']==i].drop(['cluster'],axis=1)\n",
    "  pca_components_number = len(df_new)-1 if len(df_new)<dim  else dim #It is possible to get clusters of size smaller than d. you can for a generic cluster of m points, take the\n",
    "#unique sub-space of dimension m−1 passing through these points, and get a subspace with dimension less than d.\n",
    "  pca = PCA(n_components=pca_components_number)\n",
    "  pca.fit_transform(df_new)\n",
    "  B_kmeans = pca.components_\n",
    "  return (B_kmeans.T)\n",
    "\n",
    "# apply kmeans\n",
    "def find_kmeans_subspace(X,k,dim):\n",
    "  temp_df = pd.DataFrame(X)\n",
    "  temp_df['cluster'] = KMeans(n_clusters=k).fit(X).labels_\n",
    "  B_kmean = [pca_subspace(temp_df,i,dim) for i in range(k)]\n",
    "  cluster_kmean = temp_df['cluster']\n",
    "  return  (B_kmean,cluster_kmean)\n",
    "\n",
    "#apply ensc\n",
    "def find_ensc_subspace(X,k,dim):\n",
    "  temp_df = pd.DataFrame(X)\n",
    "  temp_df['cluster']  = ElasticNetSubspaceClustering(n_clusters=k).fit(X).labels_\n",
    "\n",
    "  B_ensc = [pca_subspace(temp_df,i,dim) for i in range(k)]\n",
    "  cluster_ensc = temp_df['cluster']\n",
    "  return (B_ensc, cluster_ensc)\n",
    "\n",
    "\n"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "L-wYsYbjFfTd",
    "colab_type": "text"
   },
   "source": [
    "# **recovery performance**\n",
    "we seek good agreement between the true subspaces and the recovered subspaces. Since we can at best recover the correct subspaces up to a permutation, we enumerate over all\n",
    "possible permutations π ∈ SK, where Sk is the group of permutations over K elements, and take the best.\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "JCGgtFksaX-W",
    "colab_type": "code",
    "colab": {},
    "executionInfo": {
     "status": "ok",
     "timestamp": 1600241411777,
     "user_tz": -180,
     "elapsed": 927,
     "user": {
      "displayName": "nyueygho4 מהמיקמק",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Ggqc1SFhXUXd-_5m0Cmd6C6GdJ3rMiD1y_E2jB6gw=s64",
      "userId": "01963776509194069257"
     }
    }
   },
   "source": [
    "\n",
    "#The cost measures the angle between the original and estimated sub-spaces,with higher values achieved for smaller angle\n",
    "def performance_measure1(k,B1,B2):\n",
    "  all_per = list(it.permutations(range(k)))\n",
    "  sum_cos_angles_all_per = np.zeros(len(all_per))\n",
    "  for l, val in enumerate(all_per):\n",
    "    for i in range(k) :\n",
    "      if B2[val[i]].shape[1]>0 : # handling with empty clusters\n",
    "        sum_cos_angles_all_per[l]+= (math.cos(subspace_angles(B1[i],B2[val[i]]).max()))**2\n",
    "  cost_subspace = sum_cos_angles_all_per.max()\n",
    "  return (cost_subspace)\n",
    "\n",
    "\n",
    "\n",
    "def performance_measure2(cluster1,cluster2):\n",
    "  data = {'cluster1': cluster1,'cluster2': cluster2}\n",
    "  clusters = pd.DataFrame(data, index=range(len(cluster1)))\n",
    "  m = -1*np.array(clusters.groupby(['cluster1','cluster2']).size().unstack(fill_value=0))\n",
    "  indx, per = linear_sum_assignment(m)\n",
    "  cost_cluster = -m[indx,per].sum()/len(clusters)\n",
    "  return (cost_cluster)\n",
    "\n"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "VRLMgtU-gsxN",
    "colab_type": "code",
    "colab": {},
    "executionInfo": {
     "status": "ok",
     "timestamp": 1600241415410,
     "user_tz": -180,
     "elapsed": 769,
     "user": {
      "displayName": "nyueygho4 מהמיקמק",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Ggqc1SFhXUXd-_5m0Cmd6C6GdJ3rMiD1y_E2jB6gw=s64",
      "userId": "01963776509194069257"
     }
    }
   },
   "source": [
    "def all_process(k):\n",
    "    df = final_data_simulation(k)\n",
    "    kmean_resluts = df.apply(lambda x: find_kmeans_subspace(x['X'], k, x['dim']), axis=1)\n",
    "    df['B_kmean'] = [pair[0] for pair in kmean_resluts]\n",
    "    df['cluster_kmean']  = [pair[1] for pair in kmean_resluts]\n",
    "    ensc_resluts = df.apply(lambda x: find_ensc_subspace(x['X'], k, x['dim']), axis=1)\n",
    "    df['B_ensc'] = [pair[0] for pair in ensc_resluts]\n",
    "    df['cluster_ensc']  = [pair[1] for pair in ensc_resluts]\n",
    "    return (df)"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "P5tXkNtvahmt",
    "colab_type": "code",
    "colab": {}
   },
   "source": [
    "#apply algorithm\n",
    "measure1_kmean = pd.DataFrame()\n",
    "measure2_kmean = pd.DataFrame()\n",
    "measure1_ensc =  pd.DataFrame()\n",
    "measure2_ensc =  pd.DataFrame()\n",
    "k = 4\n",
    "for iter in range(1) :\n",
    "  df = all_process(k)\n",
    "  df.head()\n",
    "  measure1_kmean.insert(iter, \"\", df.apply (lambda x: performance_measure1(k, x['B'], x['B_kmean']), axis=1), True)\n",
    "  print(measure1_kmean)\n",
    "  measure2_kmean.insert(iter, \"\", df.apply(lambda x: performance_measure2(x['z'], x['cluster_kmean']), axis=1), True)\n",
    "  print(measure2_kmean)\n",
    "  measure1_ensc.insert(iter, \"\", df.apply(lambda x: performance_measure1(k, x['B'], x['B_ensc']), axis=1), True)\n",
    "  print(measure1_ensc)\n",
    "  measure2_ensc.insert(iter, \"\", df.apply(lambda x: performance_measure2(x['z'], x['cluster_ensc']), axis=1), True)\n",
    "  print(measure2_ensc)\n",
    "df['measure1_kmean'] = measure1_kmean.apply(lambda x: mean(x), axis=1)\n",
    "df['measure2_kmean'] = measure2_kmean.apply(lambda x: mean(x), axis=1)\n",
    "df['measure1_ensc'] = measure1_ensc.apply(lambda x: mean(x), axis=1)\n",
    "df['measure2_ensc'] = measure2_ensc.apply(lambda x: mean(x), axis=1)\n",
    "df['theta_degree'] = df.apply(lambda x: math.degrees(x['theta']), axis=1)\n",
    "df['t'] = list(np.repeat(np.array([1,1/10,1/100]), [8,8,8],axis=0))*16\n",
    "df['theta_degree'] = round(df['theta_degree'],2)\n",
    "df.to_csv('q1_df15.csv')\n",
    "files.download('q1_df15.csv')\n",
    "df.head()\n",
    "\n"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "GwK6tk4loviE",
    "colab_type": "code",
    "colab": {},
    "cellView": "form"
   },
   "source": [
    "#@title Default title text\n",
    "if 'google.colab' in sys.modules:\n",
    "    uploaded = files.upload()\n",
    "df = pd.read_csv('q1_df12 (1).csv')\n",
    "df['cluster_kmean']=df['cluster_kmean'].apply(lambda x: x.split('\\n'))\n",
    "df['cluster_ensc']=df['cluster_ensc'].apply(lambda x: x.split('\\n'))\n",
    "df['B_kmean']=df['B_kmean'].apply(lambda x: x.split('\\n'))\n",
    "df['B_ensc']=df['B_ensc'].apply(lambda x: x.split('\\n'))\n"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0sWs1KsMIqNJ",
    "colab_type": "text"
   },
   "source": [
    "## Visualization of recovery performance\n",
    "For each of the two recovery performance measures, and for each\n",
    "value of (p, d), make a heatmap showing performance as function of angle θ and\n",
    "number of samples n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "DxShDvvWdKi6",
    "colab_type": "code",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "executionInfo": {
     "status": "ok",
     "timestamp": 1600254016128,
     "user_tz": -180,
     "elapsed": 103125,
     "user": {
      "displayName": "nyueygho4 מהמיקמק",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Ggqc1SFhXUXd-_5m0Cmd6C6GdJ3rMiD1y_E2jB6gw=s64",
      "userId": "01963776509194069257"
     }
    },
    "outputId": "5f5af9a0-a73b-4840-9c9d-2a6c0f47bfea"
   },
   "source": [
    "all_measurea = [\"measure1_kmean\",\"measure2_kmean\",\"measure1_ensc\",\"measure2_ensc\"]\n",
    "fig, axes  = plt.subplots(8,8,sharex=False, sharey=False,figsize=(32,32))\n",
    "fig.suptitle('all measures for both clustering methods by p and dim', fontsize=24)\n",
    "pp = [2 ** j for j in range(4,8)]\n",
    "dd = [2 ** -j for j in range(1,5)]\n",
    "i=0\n",
    "j=0\n",
    "for p in pp:\n",
    "  for d in dd:\n",
    "    dim = int(d*p)\n",
    "    for measure in all_measurea:\n",
    "      sns_df = df[(df['p']==p) & (df['dim']==dim)]\n",
    "      sns_df = sns_df.pivot(\"theta_degree\", \"n\", measure)\n",
    "      sns.heatmap(sns_df,ax= axes[i,j])\n",
    "      plt.subplots_adjust(wspace=1,hspace = 1)\n",
    "      #counter = counter+1\n",
    "      axes[i,j].set_title('{a}: p= {b} ,dim= {c} '.format(a=measure,b=p, c=dim), fontsize=12)\n",
    "      i= i if (j<7) else i+1\n",
    "      j= j+1 if (j<7) else 0\n",
    "\n"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "h7Nh-3_tKbby",
    "colab_type": "text"
   },
   "source": [
    "average accuracy for data simulated as function of n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "ZyqMvBUDIT2y",
    "colab_type": "code",
    "colab": {},
    "executionInfo": {
     "status": "ok",
     "timestamp": 1600274047783,
     "user_tz": -180,
     "elapsed": 812,
     "user": {
      "displayName": "nyueygho4 מהמיקמק",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Ggqc1SFhXUXd-_5m0Cmd6C6GdJ3rMiD1y_E2jB6gw=s64",
      "userId": "01963776509194069257"
     }
    }
   },
   "source": [
    "  def find_accuracy_rate(n,p,dim,theta,k,iter,t):\n",
    "    accuracy_rate = []\n",
    "    for r in range(iter):\n",
    "      b = first_simulatin(p,dim,k)\n",
    "      if (t==1) :\n",
    "          a=1\n",
    "          B = [b[0]*(1-a)+b[i]*a for i in range(1,k+1)]\n",
    "      else :\n",
    "          B= second_simulation(p,k,dim,theta,b)\n",
    "      z = np.random.randint(0,k,n)  \n",
    "      w = np.random.multivariate_normal(mean = np.zeros(dim),cov =np.diag(np.ones(dim)),size=int(n))\n",
    "      X= np.zeros((n,p))\n",
    "      for i in range(n):\n",
    "        X[i,] = np.random.multivariate_normal(mean = np.array(np.dot(np.matrix(w[i,:]),B[z[i]].T)).flatten(),cov = np.diag(0*np.ones(p)))  \n",
    "      ensc_results = find_ensc_subspace(X,k,dim)\n",
    "      ensc_clusters = ensc_results[1]\n",
    "      accuracy_rate.append(performance_measure2(z,ensc_clusters))\n",
    "    avg_accuracy_rate = mean(accuracy_rate) \n",
    "    return (avg_accuracy_rate - 0.5)"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jThEAOauLx5U",
    "colab_type": "text"
   },
   "source": [
    "Estimate for each such configuration n0.5, i.e. the sample size required to cluster half\n",
    "the points correctly"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "ZaxFasXbRWWJ",
    "colab_type": "code",
    "colab": {},
    "executionInfo": {
     "status": "ok",
     "timestamp": 1600274943146,
     "user_tz": -180,
     "elapsed": 1084,
     "user": {
      "displayName": "nyueygho4 מהמיקמק",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Ggqc1SFhXUXd-_5m0Cmd6C6GdJ3rMiD1y_E2jB6gw=s64",
      "userId": "01963776509194069257"
     }
    }
   },
   "source": [
    "\n",
    "\n",
    "def binary_search(low,high,acc,p,dim,theta,k,iter,t): \n",
    "  mid = (high + low) // 2\n",
    "  value = find_accuracy_rate(mid,p,dim,theta,k,iter,t)\n",
    "  if (value <= acc)  & (value >= -1*acc): \n",
    "    return mid \n",
    "  elif  value <-acc: \n",
    "    return binary_search(low, mid - 1, acc,p,dim,theta,k,iter,t) \n",
    "  elif value >acc:\n",
    "    return binary_search(mid + 1, high, acc,p,dim,theta,k,iter,t) \n",
    "  else: \n",
    "    return -1"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "8iW9qbMXDaYf",
    "colab_type": "code",
    "colab": {}
   },
   "source": [
    "df2 = df.groupby(['p','dim','theta','t']).size().reset_index()\n",
    "df2['d\\p'] = df2['dim']/df2['p']\n",
    "df2['n_q'] = np.repeat(0,len(df2))\n",
    "for row_no in range(len(df2)): \n",
    "  p = df2['p'][row_no]\n",
    "  dim = df2['dim'][row_no]\n",
    "  t = df2['t'][row_no]\n",
    "  theta = df2['theta'][row_no]\n",
    "  df2['n_q'][row_no]= binary_search(8,1024,0.1,p,dim,theta,4,1,t)\n"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "3mO9D_2mRrNZ",
    "colab_type": "code",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 468
    },
    "executionInfo": {
     "status": "ok",
     "timestamp": 1600272670055,
     "user_tz": -180,
     "elapsed": 2158,
     "user": {
      "displayName": "nyueygho4 מהמיקמק",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Ggqc1SFhXUXd-_5m0Cmd6C6GdJ3rMiD1y_E2jB6gw=s64",
      "userId": "01963776509194069257"
     }
    },
    "outputId": "cee66c58-c3f2-4dbf-ee92-00fb47c86514"
   },
   "source": [
    "pp =  np.unique(df2['p'])\n",
    "tt =  np.unique(df2['t'])\n",
    "plt.figure(figsize=(13,7))\n",
    "newcolors = ['#F00','#F80','#FF0','#0B0','#00F','#50F','#A0F','#DC143C','#00FFFF', '#00008B','#008B8B','#B8860B']\n",
    "i=0 \n",
    "for p in np.unique(df2['p']):\n",
    "  for t in np.unique(df2['t']):\n",
    "    plt_df = df2[(df2['p']==p) & (df2['t']==t)]\n",
    "    plt.plot (plt_df['d\\p'],plt_df['n_q'],linewidth=4.0, c=newcolors[i] , label=\"p= {a},t={b}\".format(a=p, b=t))\n",
    "    i= i+1\n",
    "    plt.xlabel(\"d/p\",size=15)\n",
    "    plt.ylabel(\"n0.5\",size=15)\n",
    "    plt.title(\"dim/p VS n0.5 in ENSC method\",size=20)\n",
    "    plt.legend(loc='upper left')\n",
    "    positions = (1/16,1/8,1/4,1/2)\n",
    "    labels = (\"0.0625\", \"0.125\", \"0.25\",\"0.5\")\n",
    "    plt.xticks(positions, labels)\n",
    "\n",
    "\n",
    "\n",
    " "
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "FmlgIt8KS6d6",
    "colab_type": "code",
    "colab": {},
    "executionInfo": {
     "status": "ok",
     "timestamp": 1600274879323,
     "user_tz": -180,
     "elapsed": 812,
     "user": {
      "displayName": "nyueygho4 מהמיקמק",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Ggqc1SFhXUXd-_5m0Cmd6C6GdJ3rMiD1y_E2jB6gw=s64",
      "userId": "01963776509194069257"
     }
    }
   },
   "source": [
    "\n",
    "def find_b_constants(b_cons,df):\n",
    "  optim_df = pd.DataFrame()\n",
    "  for p in np.unique(df['p']):\n",
    "    for t in np.unique(df['t']):\n",
    "      n1,n2, n3, n4 = df['n_q'][(df['p']==p) & (df['t']==t)]\n",
    "      row=pd.Series([n1,n2,n3,n4])\n",
    "      optim_df= optim_df.append([row],ignore_index=False)\n",
    "  optim_df['b_cons'] = b_cons\n",
    "  new_df = optim_df.iloc[:,:4].apply(lambda x: (x/optim_df['b_cons'] ),axis=0)\n",
    "  #return (0 if (new_df.apply(lambda x: len(np.unique(round(x,2)))==1,axis=0)).all() else 1)\n",
    "  #return new_df.apply(lambda x: len(np.unique(round(x,2)))==1,axis=0).sum()\n",
    "  return new_df.apply(lambda x: variance(x), axis=0).sum()\n",
    "\n"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "sLC5bJ1twtTe",
    "colab_type": "code",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 67
    },
    "executionInfo": {
     "status": "ok",
     "timestamp": 1600279521105,
     "user_tz": -180,
     "elapsed": 15114,
     "user": {
      "displayName": "nyueygho4 מהמיקמק",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Ggqc1SFhXUXd-_5m0Cmd6C6GdJ3rMiD1y_E2jB6gw=s64",
      "userId": "01963776509194069257"
     }
    },
    "outputId": "9a126cd9-18b0-447e-9839-98e324a3a8a3"
   },
   "source": [
    "#f= minimize(find_b_constants, x0= np.random.randint(df2['n_q'].min(), df2['n_q'].max(), 12) ,args =(df2))\n",
    "f= minimize(find_b_constants, x0= np.random.uniform(0,1,12) ,args =(df2))\n",
    "xx=f['x']\n",
    "xx"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "4UfirJTtDvWy",
    "colab_type": "code",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 468
    },
    "executionInfo": {
     "status": "ok",
     "timestamp": 1600274839118,
     "user_tz": -180,
     "elapsed": 1173,
     "user": {
      "displayName": "nyueygho4 מהמיקמק",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Ggqc1SFhXUXd-_5m0Cmd6C6GdJ3rMiD1y_E2jB6gw=s64",
      "userId": "01963776509194069257"
     }
    },
    "outputId": "3c296343-94c2-424b-b772-a1330f345ea7"
   },
   "source": [
    "pp =  np.unique(df2['p'])\n",
    "tt =  np.unique(df2['t'])\n",
    "plt.figure(figsize=(13,7))\n",
    "newcolors = ['#F00','#F80','#FF0','#0B0','#00F','#50F','#A0F',    '#DC143C','#00FFFF', '#00008B','#008B8B','#B8860B']\n",
    "\n",
    "i=0 \n",
    "for p in np.unique(df2['p']):\n",
    "  for t in np.unique(df2['t']):\n",
    "    plt_df = df2[(df2['p']==p) & (df2['t']==t)]\n",
    "    normalized_n_q = (plt_df['n_q']/xx[i])\n",
    "    plt.plot (plt_df['d\\p'],normalized_n_q,linewidth=4.0, c=newcolors[i] , label=\"p= {a},t={b}\".format(a=p, b=t))\n",
    "\n",
    "    i= i+1\n",
    "    plt.xlabel(\"d/p\",size=15)\n",
    "    plt.ylabel(\"n0.5\",size=15)\n",
    "    plt.title(\"dim/p VS n0.5 in ENSC method\",size=20)\n",
    "    plt.legend(loc='upper left')\n",
    "    positions = (1/16,1/8,1/4,1/2)\n",
    "    labels = (\"0.0625\", \"0.125\", \"0.25\",\"0.5\")\n",
    "    plt.xticks(positions, labels)\n",
    "    "
   ],
   "execution_count": null,
   "outputs": []
  }
 ]
}